{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "47e64413",
      "metadata": {},
      "source": [
        "# Week 3 â€“ Water Segmentation with U-Net\n",
        "\n",
        "This notebook explores the provided multispectral water dataset and implements a full U-Net segmentation pipeline.\n",
        "\n",
        "Steps we will follow:\n",
        "- **Inspect the raw data** (image and label formats, shapes, basic stats)\n",
        "- **Visualize bands** and example masks\n",
        "- **Prepare a PyTorch dataset & data loaders**\n",
        "- **Define a U-Net model** for 12-channel input and 1-channel water mask output\n",
        "- **Train the model** and track IoU / precision / recall / F1 for the water class\n",
        "- **Visualize predictions** vs ground-truth masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c7e51b49",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tifffile is not installed. Run `pip install tifffile` in this environment.\n",
            "Images dir: d:\\Cellula_Internship\\Task 3\\data\\images\n",
            "Labels dir: d:\\Cellula_Internship\\Task 3\\data\\labels\n",
            "Found 306 image tiles\n",
            "Found 456 label masks (including augmented variants with underscores)\n"
          ]
        }
      ],
      "source": [
        "# Imports and basic paths\n",
        "import os\n",
        "import glob\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "import tifffile  # for reading multi-band .tif\n",
        "\n",
        "# NOTE: The .tif tiles are multispectral and PIL cannot decode them correctly.\n",
        "# We will *always* use tifffile for reading the images. Please make sure\n",
        "# `tifffile` is installed in your Python environment: `pip install tifffile`.\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "\n",
        "# Paths (adjust if you move the notebook)\n",
        "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  # points to `Task 3`\n",
        "IMAGES_DIR = os.path.join(BASE_DIR, \"data\", \"images\")\n",
        "LABELS_DIR = os.path.join(BASE_DIR, \"data\", \"labels\")\n",
        "\n",
        "print(\"Images dir:\", IMAGES_DIR)\n",
        "print(\"Labels dir:\", LABELS_DIR)\n",
        "\n",
        "image_files = sorted(glob.glob(os.path.join(IMAGES_DIR, \"*.tif\")))\n",
        "label_files = sorted(glob.glob(os.path.join(LABELS_DIR, \"*.png\")))\n",
        "\n",
        "print(f\"Found {len(image_files)} image tiles\")\n",
        "print(f\"Found {len(label_files)} label masks (including augmented variants with underscores)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "82038044",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "More samples per pixel than can be decoded: 12\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using 306 base label masks (1:1 with image indices)\n",
            "Example image: d:\\Cellula_Internship\\Task 3\\data\\images\\0.tif\n",
            "Example label: d:\\Cellula_Internship\\Task 3\\data\\labels\\0.png\n"
          ]
        },
        {
          "ename": "UnidentifiedImageError",
          "evalue": "cannot identify image file 'D:\\\\Cellula_Internship\\\\Task 3\\\\data\\\\images\\\\0.tif'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mUnidentifiedImageError\u001b[39m                    Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m     img = tifffile.imread(example_img_path)  \u001b[38;5;66;03m# expected shape: (H, W, C) or (C, H, W)\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     28\u001b[39m     \u001b[38;5;66;03m# Fallback using PIL (may not preserve all bands correctly, but useful as a check)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     pil_img = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample_img_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m     img = np.array(pil_img)\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRaw image shape:\u001b[39m\u001b[33m\"\u001b[39m, img.shape)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\George\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\Image.py:3498\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3496\u001b[39m     warnings.warn(message)\n\u001b[32m   3497\u001b[39m msg = \u001b[33m\"\u001b[39m\u001b[33mcannot identify image file \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m % (filename \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;28;01melse\u001b[39;00m fp)\n\u001b[32m-> \u001b[39m\u001b[32m3498\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m UnidentifiedImageError(msg)\n",
            "\u001b[31mUnidentifiedImageError\u001b[39m: cannot identify image file 'D:\\\\Cellula_Internship\\\\Task 3\\\\data\\\\images\\\\0.tif'"
          ]
        }
      ],
      "source": [
        "# Explore a single example (image + label)\n",
        "\n",
        "# We will use only label files whose name is an integer (no underscore),\n",
        "# so that they map directly to the corresponding image index.\n",
        "\n",
        "def is_base_label(fname: str) -> bool:\n",
        "    \"\"\"Return True if label filename (without extension) is a pure integer (e.g. '123').\"\"\"\n",
        "    stem = os.path.splitext(os.path.basename(fname))[0]\n",
        "    return stem.isdigit()\n",
        "\n",
        "base_label_files = [f for f in label_files if is_base_label(f)]\n",
        "base_label_files = sorted(base_label_files, key=lambda p: int(os.path.splitext(os.path.basename(p))[0]))\n",
        "\n",
        "print(f\"Using {len(base_label_files)} base label masks (1:1 with image indices)\")\n",
        "\n",
        "# Pick one index to inspect\n",
        "example_idx = 0\n",
        "example_img_path = os.path.join(IMAGES_DIR, f\"{example_idx}.tif\")\n",
        "example_lbl_path = os.path.join(LABELS_DIR, f\"{example_idx}.png\")\n",
        "\n",
        "print(\"Example image:\", example_img_path)\n",
        "print(\"Example label:\", example_lbl_path)\n",
        "\n",
        "# Load the multispectral tile *only* with tifffile\n",
        "img = tifffile.imread(example_img_path)  # expected shape: (H, W, C) or (C, H, W)\n",
        "\n",
        "print(\"Raw image shape:\", img.shape)\n",
        "\n",
        "# Make sure we have (C, H, W)\n",
        "if img.ndim == 3 and img.shape[0] in (12, 13):\n",
        "    img_chw = img\n",
        "elif img.ndim == 3 and img.shape[-1] in (12, 13):\n",
        "    img_chw = np.transpose(img, (2, 0, 1))\n",
        "else:\n",
        "    raise ValueError(f\"Unexpected image shape {img.shape}; please inspect this cell.\")\n",
        "\n",
        "num_channels, H, W = img_chw.shape\n",
        "print(f\"Image has {num_channels} bands, height={H}, width={W}\")\n",
        "\n",
        "# Load the corresponding label mask\n",
        "label = np.array(Image.open(example_lbl_path))\n",
        "print(\"Raw label shape:\", label.shape, \"dtype:\", label.dtype)\n",
        "\n",
        "# Ensure label is binary (0/1)\n",
        "unique_vals = np.unique(label)\n",
        "print(\"Unique label values:\", unique_vals)\n",
        "\n",
        "# If values are not 0/1, we will binarize later in the Dataset class if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "211fafad",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize a few bands and the label mask\n",
        "\n",
        "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "axes = axes.ravel()\n",
        "\n",
        "# Show up to 7 bands (0-6) + label\n",
        "max_bands_to_show = 7\n",
        "for i in range(max_bands_to_show):\n",
        "    if i >= num_channels:\n",
        "        break\n",
        "    ax = axes[i]\n",
        "    band = img_chw[i]\n",
        "    im = ax.imshow(band, cmap=\"gray\")\n",
        "    ax.set_title(f\"Band {i}\")\n",
        "    ax.axis(\"off\")\n",
        "    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "\n",
        "# Last subplot: label mask\n",
        "ax_lbl = axes[-1]\n",
        "ax_lbl.imshow(label, cmap=\"gray\")\n",
        "ax_lbl.set_title(\"Label mask\")\n",
        "ax_lbl.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d320c5e8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# PyTorch Dataset for water segmentation\n",
        "\n",
        "class WaterSegmentationDataset(Dataset):\n",
        "    def __init__(self, images_dir, labels_dir, base_label_files, normalize=True):\n",
        "        self.images_dir = images_dir\n",
        "        self.labels_dir = labels_dir\n",
        "        self.normalize = normalize\n",
        "\n",
        "        # Build pairs (image_path, label_path) using base (numeric) label filenames\n",
        "        self.samples = []\n",
        "        for lbl_path in base_label_files:\n",
        "            stem = os.path.splitext(os.path.basename(lbl_path))[0]  # e.g. '123'\n",
        "            img_path = os.path.join(images_dir, f\"{stem}.tif\")\n",
        "            if os.path.exists(img_path):\n",
        "                self.samples.append((img_path, lbl_path))\n",
        "\n",
        "        if len(self.samples) == 0:\n",
        "            raise RuntimeError(\"No (image, label) pairs found. Check your folder structure.\")\n",
        "\n",
        "        print(f\"Dataset initialized with {len(self.samples)} samples.\")\n",
        "\n",
        "        # Optionally pre-compute mean/std per channel on the fly for normalization\n",
        "        # (for simplicity, we skip this here and do simple scaling in __getitem__).\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def _load_image(self, path):\n",
        "        # Always use tifffile for multispectral tiles; PIL cannot read these .tif files reliably.\n",
        "        img = tifffile.imread(path)\n",
        "\n",
        "        if img.ndim == 3 and img.shape[0] in (12, 13):\n",
        "            img_chw = img\n",
        "        elif img.ndim == 3 and img.shape[-1] in (12, 13):\n",
        "            img_chw = np.transpose(img, (2, 0, 1))\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected image shape {img.shape} for {path}\")\n",
        "\n",
        "        img_chw = img_chw.astype(np.float32)\n",
        "\n",
        "        # Simple normalization: scale to [0, 1] by global max if needed\n",
        "        if self.normalize:\n",
        "            max_val = img_chw.max() if img_chw.max() > 0 else 1.0\n",
        "            img_chw = img_chw / max_val\n",
        "\n",
        "        return img_chw\n",
        "\n",
        "    def _load_label(self, path):\n",
        "        lbl = np.array(Image.open(path))\n",
        "        # Ensure we have a single channel mask\n",
        "        if lbl.ndim == 3:\n",
        "            # if RGB, convert to single channel (assuming water is white)\n",
        "            lbl = lbl[..., 0]\n",
        "\n",
        "        # Binarize: any non-zero is water\n",
        "        lbl_bin = (lbl > 0).astype(np.float32)\n",
        "        return lbl_bin\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, lbl_path = self.samples[idx]\n",
        "        img = self._load_image(img_path)\n",
        "        lbl = self._load_label(lbl_path)\n",
        "\n",
        "        # Convert to torch tensors\n",
        "        img_tensor = torch.from_numpy(img)              # (C, H, W)\n",
        "        lbl_tensor = torch.from_numpy(lbl).unsqueeze(0) # (1, H, W)\n",
        "\n",
        "        return img_tensor, lbl_tensor\n",
        "\n",
        "\n",
        "# Instantiate dataset\n",
        "full_dataset = WaterSegmentationDataset(IMAGES_DIR, LABELS_DIR, base_label_files)\n",
        "\n",
        "# Simple train/val split\n",
        "val_fraction = 0.2\n",
        "val_size = int(len(full_dataset) * val_fraction)\n",
        "train_size = len(full_dataset) - val_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "print(f\"Train size: {len(train_dataset)}, Val size: {len(val_dataset)}\")\n",
        "\n",
        "batch_size = 4\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02634766",
      "metadata": {},
      "outputs": [],
      "source": [
        "# U-Net model definition (12-channel input -> 1-channel water mask)\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_ch=12, out_ch=1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.down1 = DoubleConv(in_ch, 64)\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.down2 = DoubleConv(64, 128)\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.down3 = DoubleConv(128, 256)\n",
        "        self.pool3 = nn.MaxPool2d(2)\n",
        "        self.down4 = DoubleConv(256, 512)\n",
        "        self.pool4 = nn.MaxPool2d(2)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = DoubleConv(512, 1024)\n",
        "\n",
        "        # Decoder\n",
        "        self.up4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
        "        self.dec4 = DoubleConv(1024, 512)\n",
        "        self.up3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.dec3 = DoubleConv(512, 256)\n",
        "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.dec2 = DoubleConv(256, 128)\n",
        "        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.dec1 = DoubleConv(128, 64)\n",
        "\n",
        "        self.out_conv = nn.Conv2d(64, out_ch, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        c1 = self.down1(x)\n",
        "        p1 = self.pool1(c1)\n",
        "\n",
        "        c2 = self.down2(p1)\n",
        "        p2 = self.pool2(c2)\n",
        "\n",
        "        c3 = self.down3(p2)\n",
        "        p3 = self.pool3(c3)\n",
        "\n",
        "        c4 = self.down4(p3)\n",
        "        p4 = self.pool4(c4)\n",
        "\n",
        "        # Bottleneck\n",
        "        bn = self.bottleneck(p4)\n",
        "\n",
        "        # Decoder with skip connections\n",
        "        u4 = self.up4(bn)\n",
        "        u4 = torch.cat([u4, c4], dim=1)\n",
        "        c5 = self.dec4(u4)\n",
        "\n",
        "        u3 = self.up3(c5)\n",
        "        u3 = torch.cat([u3, c3], dim=1)\n",
        "        c6 = self.dec3(u3)\n",
        "\n",
        "        u2 = self.up2(c6)\n",
        "        u2 = torch.cat([u2, c2], dim=1)\n",
        "        c7 = self.dec2(u2)\n",
        "\n",
        "        u1 = self.up1(c7)\n",
        "        u1 = torch.cat([u1, c1], dim=1)\n",
        "        c8 = self.dec1(u1)\n",
        "\n",
        "        logits = self.out_conv(c8)\n",
        "        return logits\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "model = UNet(in_ch=img_chw.shape[0], out_ch=1).to(device)\n",
        "print(\"Model parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db4d6725",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loss, optimizer, and metrics\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "def compute_metrics(preds, targets, eps: float = 1e-7):\n",
        "    \"\"\"Compute IoU, precision, recall, F1 for the water class.\n",
        "\n",
        "    preds and targets are torch tensors of shape (B, 1, H, W) with values 0/1.\n",
        "    \"\"\"\n",
        "    preds = preds.view(-1)\n",
        "    targets = targets.view(-1)\n",
        "\n",
        "    tp = torch.sum((preds == 1) & (targets == 1)).float()\n",
        "    fp = torch.sum((preds == 1) & (targets == 0)).float()\n",
        "    fn = torch.sum((preds == 0) & (targets == 1)).float()\n",
        "\n",
        "    precision = tp / (tp + fp + eps)\n",
        "    recall = tp / (tp + fn + eps)\n",
        "    f1 = 2 * precision * recall / (precision + recall + eps)\n",
        "    iou = tp / (tp + fp + fn + eps)\n",
        "\n",
        "    return {\n",
        "        \"precision\": precision.item(),\n",
        "        \"recall\": recall.item(),\n",
        "        \"f1\": f1.item(),\n",
        "        \"iou\": iou.item(),\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6eaeadb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop\n",
        "\n",
        "num_epochs = 10  # adjust as needed\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for images, masks in train_loader:\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        logits = model(images)\n",
        "        loss = criterion(logits, masks)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    all_metrics = {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0, \"iou\": 0.0}\n",
        "    num_val_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in val_loader:\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            logits = model(images)\n",
        "            loss = criterion(logits, masks)\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "\n",
        "            probs = torch.sigmoid(logits)\n",
        "            preds = (probs > 0.5).float()\n",
        "\n",
        "            metrics = compute_metrics(preds, masks)\n",
        "            for k in all_metrics.keys():\n",
        "                all_metrics[k] += metrics[k]\n",
        "            num_val_batches += 1\n",
        "\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    for k in all_metrics.keys():\n",
        "        all_metrics[k] /= max(1, num_val_batches)\n",
        "\n",
        "    print(f\"Epoch {epoch}/{num_epochs} | \"\n",
        "          f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "          f\"IoU: {all_metrics['iou']:.4f} | F1: {all_metrics['f1']:.4f} | \"\n",
        "          f\"Prec: {all_metrics['precision']:.4f} | Rec: {all_metrics['recall']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8484d18",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize predictions vs ground-truth for a few validation samples\n",
        "\n",
        "model.eval()\n",
        "\n",
        "n_visualize = 3\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, (images, masks) in enumerate(val_loader):\n",
        "        if i >= n_visualize:\n",
        "            break\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        logits = model(images)\n",
        "        probs = torch.sigmoid(logits)\n",
        "        preds = (probs > 0.5).float()\n",
        "\n",
        "        # Move to CPU for plotting\n",
        "        images_np = images.cpu().numpy()\n",
        "        masks_np = masks.cpu().numpy()\n",
        "        preds_np = preds.cpu().numpy()\n",
        "\n",
        "        batch_size_vis = images_np.shape[0]\n",
        "\n",
        "        for b in range(batch_size_vis):\n",
        "            img_chw = images_np[b]\n",
        "            gt = masks_np[b, 0]\n",
        "            pr = preds_np[b, 0]\n",
        "\n",
        "            # Simple visualization: take 3 bands to form an RGB-like image (if at least 3 bands)\n",
        "            if img_chw.shape[0] >= 3:\n",
        "                rgb = np.stack([\n",
        "                    img_chw[0],\n",
        "                    img_chw[1],\n",
        "                    img_chw[2],\n",
        "                ], axis=-1)\n",
        "                # Normalize to [0,1] for display\n",
        "                rgb = (rgb - rgb.min()) / (rgb.max() - rgb.min() + 1e-7)\n",
        "            else:\n",
        "                rgb = img_chw[0]\n",
        "\n",
        "            fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "            axes[0].imshow(rgb)\n",
        "            axes[0].set_title(\"Input (3 bands)\")\n",
        "            axes[0].axis(\"off\")\n",
        "\n",
        "            axes[1].imshow(gt, cmap=\"gray\")\n",
        "            axes[1].set_title(\"Ground truth mask\")\n",
        "            axes[1].axis(\"off\")\n",
        "\n",
        "            axes[2].imshow(pr, cmap=\"gray\")\n",
        "            axes[2].set_title(\"Predicted mask\")\n",
        "            axes[2].axis(\"off\")\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
